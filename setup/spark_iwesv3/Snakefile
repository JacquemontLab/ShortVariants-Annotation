
# Activate the virtual environment containing snakemake

import os

# Access config variables
# Define project paths
pathWorkflow = config["pathWorkflow"]

SAMPLES = config["sample_list"]
GVCF_PATHS = config["gvcf_paths"]

NUM_BATCHES = config["num_batches"]

# Check if NUM_BATCHES exceeds the maximum allowed value (10,000)
if NUM_BATCHES > 10000:
    raise ValueError(f"NUM_BATCHES is set to {NUM_BATCHES}, which exceeds the maximum allow

FASTA_REF = config["pathWorkflow"] + "resources/reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa.gz"


pathVEPcache = pathWorkflow + "/resources/vep_cache/"  # Main project directory
pathResults = pathWorkflow + "/results/"  # Main project directory
path_venv = pathWorkflow + "/software/venv/"  # Path to the virtual environment
pathScripts = pathWorkflow + "/scripts/"  # Directory containing workflow scripts
pathLog = pathResults  + "/logs/"  # Directory for log files

pathTempResults = pathWorkflow + "/results/tmp/"  # Main project directory

# Create directories if they don't exist
for path in [pathWorkflow, pathResults, path_venv, pathScripts, pathLog]:
    os.makedirs(path, exist_ok=True)


# List all log subdirectories used by rules
log_subdirs = [
    "produce_tsv_per_sample_SPARK",
    "merge_tsv_parquet_by_batch",
    "plugins/run_vep_default", 
    "plugins/run_vep_loftee", 
    "plugins/run_vep_alphamissense", 
    "plugins/run_vep_spliceai", 
]

# Create each log directory
for subdir in log_subdirs:
    full_path = os.path.join(pathLog, subdir)
    os.makedirs(full_path, exist_ok=True)



localrules: split_sample_list  # Ensures this rule runs locally


PLUGINS = ["spliceai","alphamissense","loftee"]  # Define annotation plugins to use
CHROMOSOMES = [f"chr{i}" for i in range(1, 23)] + ["chrX", "chrY"]  # List of chromosomes to process


rule all:
    input:
        expand(pathResults  + "/Refined_Annotated_SNVs.parquet"),
        expand(pathResults  + "/Lossless_Annotated_SNVs.parquet"),
        expand(pathResults  + "/Refined_Annotated_SNVs_dictionary.pdf"),
        expand(pathResults  + "/Lossless_Annotated_SNVs_dictionary.pdf")



rule split_sample_list:
    # This rule splits a large sample list into multiple smaller files (batches) 
    # to facilitate parallel processing. Each batch contains approximately equal 
    # numbers of lines, ensuring balanced workload distribution.
    params:
        num_batches = NUM_BATCHES
    input:
        sample_list = SAMPLES
    output:
        batches = directory(pathTempResults + "data/inputs/batches/"),
        files = expand(pathTempResults + "data/inputs/batches/batch_{i:04d}", i=range(NUM_BATCHES))
    shell:
        """
        mkdir -p {output.batches}
        # Count total lines
        total_lines=$(wc -l < {input.sample_list})

        # Determine number of lines per batch
        lines_per_batch=$(( (total_lines + {params.num_batches} - 1) / {params.num_batches} ))
        
        # Split the file into num_batches files
        split -d -a 4 -l $lines_per_batch {input.sample_list} {output.batches}/batch_
        """


rule produce_tsv_per_sample_SPARK:
    # This rule processes per-sample gVCF files for the SPARK dataset for a given batch.
    # It extracts SNPs, INDELs, and heterozygous sites from GATK and DeepVariant variant calls,
    # retaining only shared SNVs while keeping DeepVariant metadata.
    # The processed data is saved as a compressed TSV file for each sample.
    #
    # Note: This script does utilize multiple threads with GNU-parallel.
    resources:
        time = "00-00:30:00", mem_per_cpu = 3, cpus = 64
    params:
        name = "produce_tsv_per_sample_SPARK_batch_{batch}",
        stdout = pathLog + "produce_tsv_per_sample_SPARK/batch_{batch}.stdout",
        stderr = pathLog + "produce_tsv_per_sample_SPARK/batch_{batch}.stderr",
        dir_gvcf= pathTempResults + "data/processed/gvcf_per_sample/batch_{batch}/"
    input: 
        file_gvcf_path = GVCF_PATHS,
        list_sample_to_process = pathTempResults + "data/inputs/batches/batch_{batch}",
        fasta_ref = FASTA_REF
    output:
        batch_done = touch(pathTempResults + "data/processed/step/batch_{batch}_gvcf_produced")
    shell:
        """
        mkdir -p {pathTempResults}data/processed/gvcf_per_sample/batch_{wildcards.batch}/

        source {path_venv}bin/activate
        
        # Check if bcftools is available
        if ! command -v bcftools &> /dev/null; then
            echo "bcftools not found — loading module..."
            module load bcftools
        else
            echo "bcftools already available."
        fi


        process_sample() {{
            sample=$1
            input_gatk=$(zgrep "${{sample}}" "{input.file_gvcf_path}" | grep gatk | cut -f2)
            input_deepvariant=$(zgrep "${{sample}}" "{input.file_gvcf_path}" | grep deepvariant | cut -f2)
            
            /usr/bin/time -v bash -c "
                {pathScripts}dataset_specific/extraction_snps_indels_SPARK_iWESv3.sh \
                    \"${{sample}}\" \
                    \"${{input_gatk}}\" \
                    \"${{input_deepvariant}}\" \
                    \"{params.dir_gvcf}${{sample}}.tsv.gz\" \
                    \"{input.fasta_ref}\" \
                    \"{resources.cpus}\""
        }}

        export -f process_sample

        # Use GNU parallel to run multiple samples in parallel
        cat {input.list_sample_to_process} | parallel -j {resources.cpus} process_sample {{}}

        # Mark the batch as complete
        mkdir -p {pathTempResults}data/processed/step/
        """


rule merge_tsv_parquet_by_batch:
    # This rule merges multiple TSV files containing gVCF data into a single Parquet file for a given batch.
    # Note : 1sec per samples, PySpark is able to multithread but it might not necessarily changed the process time
    resources:
        time = "00-00:30:00", mem_per_cpu = 3, cpus = 2
    params:
        name = "merge_tsv_parquet_batch_{batch}",
        stdout = pathLog + "merge_tsv_parquet_by_batch/batch_{batch}.stdout",
        stderr = pathLog + "merge_tsv_parquet_by_batch/batch_{batch}.stderr",
        dir_gvcf = pathTempResults + "data/processed/gvcf_per_sample/batch_{batch}/"
    input: 
        batch_done = pathTempResults  + "data/processed/step/batch_{batch}_gvcf_produced"
    output:
        parquet_including_all_snv = directory(pathTempResults + "data/processed/intermediate_parquet/batches/Unannotated_SNVs_batch_{batch}.parquet")
    shell:
        """
        mkdir -p {pathTempResults}data/processed/intermediate_parquet/batches/

        source {path_venv}bin/activate

        /usr/bin/time -v python {pathScripts}generate_parquet_all_snvs.py {params.dir_gvcf} {output.parquet_including_all_snv} {resources.cpus} {resources.mem_per_cpu}
        """



rule merge_batches:
    # This rule merges multiple batch-level Parquet files into a single consolidated Parquet file.
    # Note : 1sec per samples, PySpark is able to multithread but it might not necessarily changed the process time
    resources:
        time = "00-03:00:00", mem_per_cpu = 3, cpus = 64
    params:
        name = "merge_batches",
        stdout = pathLog + "merge_batches.stdout",
        stderr = pathLog + "merge_batches.stderr",
        batches_files = ",".join(expand(pathTempResults + "data/processed/intermediate_parquet/batches/Unannotated_SNVs_batch_{i:04d}.parquet", i=range(NUM_BATCHES)))
    input:
        expand(pathTempResults  + "data/processed/intermediate_parquet/batches/Unannotated_SNVs_batch_{i:04d}.parquet", i=range(NUM_BATCHES))
    output:
        parquet_file = directory(pathTempResults  + "data/processed/Unannotated_SNVs.parquet")
    shell:
        """
        source {path_venv}bin/activate

        /usr/bin/time -v python {pathScripts}merge_parquets.py {params.batches_files} {output.parquet_file} {resources.cpus} {resources.mem_per_cpu}
        """



rule find_uniq_snvs_vcf:
    # This rule extracts unique SNVs from a large Parquet file and generates chromosome-specific VCF files
    resources:
        time = "00-00:30:00", mem_per_cpu = 3, cpus = 64
    params:
        name = "find_uniq_snvs_vcf",
        stdout = pathLog + "find_uniq_snvs_vcf.stdout",
        stderr = pathLog + "find_uniq_snvs_vcf.stderr",
        output_dir = pathTempResults  + "data/processed/vcf_uniq_snvs/"
    input: 
        parquet_including_all_snv = pathTempResults  + "data/processed/Unannotated_SNVs.parquet"
    output:
        expand( pathTempResults  + "data/processed/vcf_uniq_snvs/{chrom}.vcf.gz", chrom=CHROMOSOMES)
    shell:
        """
        mkdir -p {pathTempResults}data/processed/vcf_uniq_snvs/
        rm -rf {pathTempResults}data/processed/vcf_uniq_snvs/*

        source {path_venv}bin/activate

        /usr/bin/time -v python {pathScripts}generate_parquet_uniq_snvs.py {input.parquet_including_all_snv} {params.output_dir} {resources.cpus} {resources.mem_per_cpu}
        
        # Check and load bcftools if missing
        if ! command -v bcftools &> /dev/null; then
            echo "bcftools not found — loading module..."
            module load bcftools
        else
            echo "bcftools already available."
        fi

        # Check and load vcftools if missing
        if ! command -v vcftools &> /dev/null; then
            echo "vcftools not found — loading module..."
            module load vcftools
        else
            echo "vcftools already available."
        fi
        
        for file in {pathTempResults}data/processed/vcf_uniq_snvs/chr*.vcf; do
            # Sort the VCF for faster VEP processing
            vcf-sort ${{file}} > ${{file%.vcf}}_sorted.vcf

            mv ${{file%.vcf}}_sorted.vcf ${{file}}

            # Convert to .gz and index
            bgzip -c ${{file}} > ${{file}}.gz
            tabix -p vcf ${{file}}.gz
            
            # Remove the uncompressed VCF to save space
            rm ${{file}}
        done
        """



rule run_vep_default:
    # This rule runs the Ensembl Variant Effect Predictor (VEP) to annotate unique SNVs using default parameters.
    # Consequence, CANONICAL, MANE, MAX_AF, MAX_AF_POPS, gnomADe_*, gnomADg_*
    resources:
        time = "00-02:00:00", mem_per_cpu = 3, cpus = 64
    params:
        name = "run_vep_default_{chrom}",
        stdout = pathLog + "plugins/run_vep_default/{chrom}.stdout",
        stderr = pathLog + "plugins/run_vep_default/{chrom}.stderr",
    input: 
        vcf_to_annot = pathTempResults  + "data/processed/vcf_uniq_snvs/{chrom}.vcf.gz"
    output:
        tsv_vep = pathTempResults  + "data/processed/vep_annotation/default/{chrom}.tsv.gz",
        stat = pathTempResults  + "data/processed/vep_annotation/default/{chrom}.txt",
        stat_html = pathTempResults  + "data/processed/vep_annotation/default/{chrom}.html"
    shell:
        """
        mkdir -p {pathTempResults}data/processed/vep_annotation/default/

        # Check if apptainer is available
        if ! command -v apptainer &> /dev/null; then
            echo "apptainer not found — loading module..."
            module load apptainer
        else
            echo "apptainer already available."
        fi

        # Default parameters
        # Call docker VEP
        /usr/bin/time -v apptainer exec --bind {pathWorkflow}:{pathWorkflow} \
            {pathVEPcache}ensembl-vep.sif \
            vep -i {input.vcf_to_annot} --format vcf \
                --cache --offline --fork {resources.cpus} \
                --dir_cache={pathVEPcache} \
                --assembly GRCh38 \
                --force_overwrite --compress_output gzip --tab \
                --output_file {output.tsv_vep} \
                --stats_text --stats_html --stats_file {output.stat} \
                --canonical --mane --max_af --af_gnomade --af_gnomadg\
                --fields "Uploaded_variation,Location,Allele,Gene,Feature,Consequence,CANONICAL,MANE,MAX_AF,MAX_AF_POPS,gnomADe_AF,gnomADe_AFR_AF,gnomADe_AMR_AF,gnomADe_ASJ_AF,gnomADe_EAS_AF,gnomADe_FIN_AF,gnomADe_MID_AF,gnomADe_NFE_AF,gnomADe_REMAINING_AF,gnomADe_SAS_AF,gnomADg_AF,gnomADg_AFR_AF,gnomADg_AMI_AF,gnomADg_AMR_AF,gnomADg_ASJ_AF,gnomADg_EAS_AF,gnomADg_FIN_AF,gnomADg_MID_AF,gnomADg_NFE_AF,gnomADg_REMAINING_AF,gnomADg_SAS_AF" \
                --verbose
        """



rule run_vep_plugin_loftee:
    # Runs Ensembl VEP with the LOFTEE plugin to predict Loss-Of-Function of SNVs.
    resources:
        time = "00-01:00:00", mem_per_cpu = 3, cpus = 64
    params:
        name = "run_vep_plugin_loftee_{chrom}",
        stdout = pathLog + "plugins/run_vep_loftee/{chrom}.stdout",
        stderr = pathLog + "plugins/run_vep_loftee/{chrom}.stderr",
    input: 
        vcf_to_annot = pathTempResults  + "data/processed/vcf_uniq_snvs/{chrom}.vcf.gz"
    output:
        tsv_vep = pathTempResults  + "data/processed/vep_annotation/loftee/{chrom}.tsv.gz",
        stat = pathTempResults  + "data/processed/vep_annotation/loftee/{chrom}.txt",
        stat_html = pathTempResults  + "data/processed/vep_annotation/loftee/{chrom}.html"
    shell:
        """
        mkdir -p {pathTempResults}data/processed/vep_annotation/loftee/

        # Check if apptainer is available
        if ! command -v apptainer &> /dev/null; then
            echo "apptainer not found — loading module..."
            module load apptainer
        else
            echo "apptainer already available."
        fi

        # Default parameters
        # Call docker VEP
        /usr/bin/time -v apptainer exec --bind {pathWorkflow}:{pathWorkflow} \
            {pathVEPcache}ensembl-vep.sif \
            vep -i {input.vcf_to_annot} --format vcf \
                --cache --offline --fork {resources.cpus} \
                --dir_cache={pathVEPcache} \
                --assembly GRCh38 \
                --force_overwrite --compress_output gzip --tab \
                --output_file {output.tsv_vep} \
                --stats_text --stats_html --stats_file {output.stat} \
                --plugin LoF,loftee_path:{pathVEPcache}/ressources_loftee/loftee-1.0.4_GRCh38/ \
                --dir_plugins {pathVEPcache}/ressources_loftee/loftee-1.0.4_GRCh38/ \
                --fields "Uploaded_variation,Location,Allele,Gene,Feature,LoF,LoF_filter,LoF_flags,LoF_info" \
                --verbose
        """




rule run_vep_plugin_alphamissense:
    # Runs Ensembl VEP with the AlphaMissense plugin to predict missense effects of SNVs.
    resources:
        time = "00-01:00:00", mem_per_cpu = 3, cpus = 64
    params:
        name = "run_vep_plugin_alphamissense_{chrom}",
        stdout = pathLog + "plugins/run_vep_alphamissense/{chrom}.stdout",
        stderr = pathLog + "plugins/run_vep_alphamissense/{chrom}.stderr",
    input: 
        vcf_to_annot = pathTempResults  + "data/processed/vcf_uniq_snvs/{chrom}.vcf.gz"
    output:
        tsv_vep = pathTempResults  + "data/processed/vep_annotation/alphamissense/{chrom}.tsv.gz",
        stat = pathTempResults  + "data/processed/vep_annotation/alphamissense/{chrom}.txt",
        stat_html = pathTempResults  + "data/processed/vep_annotation/alphamissense/{chrom}.html"
    shell:
        """
        mkdir -p {pathTempResults}data/processed/vep_annotation/alphamissense/

        # Check if apptainer is available
        if ! command -v apptainer &> /dev/null; then
            echo "apptainer not found — loading module..."
            module load apptainer
        else
            echo "apptainer already available."
        fi

        # Default parameters
        # Call docker VEP
        /usr/bin/time -v apptainer exec --bind {pathWorkflow}:{pathWorkflow} \
            {pathVEPcache}ensembl-vep.sif \
            vep -i {input.vcf_to_annot} --format vcf \
                --cache --offline --fork {resources.cpus} \
                --dir_cache={pathVEPcache} \
                --assembly GRCh38 \
                --force_overwrite --compress_output gzip --tab \
                --output_file {output.tsv_vep} \
                --stats_text --stats_html --stats_file {output.stat} \
                --plugin AlphaMissense,file={pathVEPcache}/ressources_alphamissense/AlphaMissense_hg38.tsv.gz \
                --fields "Uploaded_variation,Location,Allele,Gene,Feature,am_class,am_pathogenicity" \
                --verbose
        """



rule run_vep_plugin_spliceai:
    # Runs Ensembl VEP with the SpliceAI plugin to predict splice-altering effects of SNVs.
    resources:
        time = "00-01:00:00", mem_per_cpu = 3, cpus = 64
    params:
        name = "run_vep_spliceai_{chrom}",
        stdout = pathLog + "plugins/run_vep_spliceai/{chrom}.stdout",
        stderr = pathLog + "plugins/run_vep_spliceai/{chrom}.stderr",
    input: 
        vcf_to_annot = pathTempResults  + "data/processed/vcf_uniq_snvs/{chrom}.vcf.gz"
    output:
        tsv_vep = pathTempResults  + "data/processed/vep_annotation/spliceai/{chrom}.tsv.gz",
        stat = pathTempResults  + "data/processed/vep_annotation/spliceai/{chrom}.txt",
        stat_html = pathTempResults  + "data/processed/vep_annotation/spliceai/{chrom}.html"
    shell:
        """
        mkdir -p {pathTempResults}data/processed/vep_annotation/spliceai/

        # Check if apptainer is available
        if ! command -v apptainer &> /dev/null; then
            echo "apptainer not found — loading module..."
            module load apptainer
        else
            echo "apptainer already available."
        fi

        # Default parameters
        # Call docker VEP
        /usr/bin/time -v apptainer exec --bind {pathWorkflow}:{pathWorkflow} \
            {pathVEPcache}software/ensembl-vep.sif \
            vep -i {input.vcf_to_annot} --format vcf \
                --cache --offline --fork {resources.cpus} \
                --dir_cache={pathVEPcache} \
                --assembly GRCh38 \
                --force_overwrite --compress_output gzip --tab \
                --output_file {output.tsv_vep} \
                --stats_text --stats_html --stats_file {output.stat} \
                --plugin SpliceAI,snv={pathVEPcache}/ressources_spliceai/spliceai_scores.raw.snv.hg38.vcf.gz,indel={pathVEPcache}/ressources_spliceai/spliceai_scores.raw.indel.hg38.vcf.gz \
                --fields "Uploaded_variation,Location,Allele,Gene,Feature,SpliceAI_pred" \
                --verbose
        """



rule convert_vep_out_parquet:
    # This rule converts VEP annotation output from compressed TSV format to Parquet, optimizing storage and retrieval.
    resources:
        time = "00-00:15:00", mem_per_cpu = 3, cpus = 64
    params:
        name = "convert_vep_out_parquet_{plugin}",
        stdout = pathLog + "convert_vep_out_parquet_{plugin}.stdout",
        stderr = pathLog + "convert_vep_out_parquet_{plugin}.stderr",
        input_vcf_dir = pathTempResults  + "data/processed/vep_annotation/{plugin}/"
    input: 
        lambda wildcards: expand(pathTempResults + "data/processed/vep_annotation/" + wildcards.plugin + "/{chrom}.tsv.gz", chrom=CHROMOSOMES)
    output:
        plugin_parquet = directory(pathTempResults  + "data/processed/intermediate_parquet/plugins/{plugin}.parquet")
    shell:
        """
        mkdir -p {pathTempResults}data/processed/intermediate_parquet/plugins/

        source {path_venv}bin/activate

        /usr/bin/time -v python {pathScripts}convert_vep_output_parquet.py {params.input_vcf_dir} {output.plugin_parquet} {resources.cpus} {resources.mem_per_cpu} {wildcards.plugin}
        """




rule lossless_annotation:
    # This rule annotates all SNVs by merging unannotated SNVs with VEP default annotations and plugin-based annotations of unique SNVs.
    resources:
        time = "00-07:00:00", mem_per_cpu = 30, cpus = 64
    params:
        name = "lossless_annotation",
        stdout = pathLog + "lossless_annotation.stdout",
        stderr = pathLog + "lossless_annotation.stderr",
        plugin_files = ",".join(expand(pathTempResults + "data/processed/intermediate_parquet/plugins/{plugin}.parquet", plugin=PLUGINS)) 
    input:
        expand( pathTempResults  + "data/processed/intermediate_parquet/plugins/{plugin}.parquet", plugin=PLUGINS),
        vep_default_path = pathTempResults  + "data/processed/intermediate_parquet/plugins/default.parquet",
        parquet_including_all_snv = pathTempResults  + "data/processed/Unannotated_SNVs.parquet"
    output:
        snvs_annotated_parquet = directory(pathResults  + "Lossless_Annotated_SNVs.parquet")
    shell:
        """
        source {path_venv}bin/activate

        /usr/bin/time -v python {pathScripts}lossless_annotation.py {input.parquet_including_all_snv} {input.vep_default_path} {params.plugin_files} {output.snvs_annotated_parquet} {resources.cpus} {resources.mem_per_cpu}
        """


rule refined_annotation:
    # This rule produces a filtered parquet file commonly used for downstream analysis.
    resources:
        time = "00-02:00:00", mem_per_cpu = 3, cpus = 64
    params:
        name = "refined_annotation",
        stdout = pathLog + "refined_annotation.stdout",
        stderr = pathLog + "refined_annotation.stderr",
    input:
        snvs_annotated_parquet = pathResults  + "Lossless_Annotated_SNVs.parquet"
    output:
        summary_report = pathResults  + "Refined_Annotated_SNVs_summary.txt",
        refined_parquet = directory(pathResults  + "Refined_Annotated_SNVs.parquet")
    shell:
        """
        source {path_venv}bin/activate

        /usr/bin/time -v python {pathScripts}refine_annotation.py {input.snvs_annotated_parquet} {output.summary_report} {output.refined_parquet} {resources.cpus} {resources.mem_per_cpu}
        """



rule produce_summary_pdf:
    # This rule produces a PDF file representing the distribution of each variable in the database.
    resources:
        time = "00-00:30:00", mem_per_cpu = 3, cpus = 64
    params:
        name = "produce_pdf_{file}",
        stdout = pathLog + "produce_pdf_{file}.stdout",
        stderr = pathLog + "produce_pdf_{file}.stderr",
    input: 
        snvs_annotated_parquet = pathResults  + "{file}.parquet"
    output:
        pdf_file = pathResults  + "{file}_dictionary.pdf"
    shell:
        """
        source {path_venv}bin/activate

        /usr/bin/time -v python {pathScripts}pdf_dictionnary.py {input.snvs_annotated_parquet} {resources.cpus} {resources.mem_per_cpu}
        """
